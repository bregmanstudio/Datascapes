{
 "metadata": {
  "name": "",
  "signature": "sha256:57918f3f09c15d84dca8251b5819890c0f1b41568c64ff0df341f2a6fb40ac1a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from pylab import *\n",
      "from bregman.suite import *\n",
      "import datascapesutils as ds # We have our own datascapes toolbox, import as module for help(ds)\n",
      "from datascapesutils import * # Bring all of these functions into the current workspace's namespace\n",
      "%matplotlib inline\n",
      "rcParams['figure.figsize'] = (10.0, 8.0) # Change defauly figure size to larger"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "##Mapping Data to an Audio Signal\n",
      "\n",
      "\n",
      "In Short Assignment 1 (SA1) you acquired some kp-index data via GitHub, parsed the data using Python's pandas module. You used pandas to perform some filtering, plotting, and mining on the data to discover patterns. You also made a simple computer music instrument and used it to hear the data at multiple different time scales. \n",
      "\n",
      "This week you will begin mapping data from source, through data mining, and mapping to audio and visual signals for display.\n",
      "The methods are to be taken only as a guide, as a way to get started. There are a vast range of methods available to us for mapping from data to audio-visual experiences, so you are strongly encouraged to explore beyond what is provided here. \n",
      "\n",
      "This week we will use one of the first methods that was developed for generating sound from data, that is *additive synthesis*.\n",
      "To do additive synthesis, we first need to understand audio signals and how to manipulate them.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sample rate, Nyquist frequency, Resampling\n",
      "\n",
      "We can listen to one-dimensinal time-series as raw audio by choosing the sample rate, which dertmines the frequency of audio playback. We choose a Nyquist frequency for human hearing, 20kHz, which is the highest frequency we wish to have in our audio signal. Assign a samplerate (`sr`) of at least double the Nyquist frequency. So, we choose 44.1kHz, which is standard for digital audio. \n",
      "\n",
      "If the playback sample rate is lower, the frequency content will be shifted lower\n",
      "If the playback sample rate is higher, the frequency content will be shifted higher\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Listen to random (normally distributed) data as audio. We can hear the periodic components of in the data\n",
      "sr = 44100 # samples per second (Hz)\n",
      "signal = randn(sr * 2) # generate 2 seconds of NORMALLY DISTRIBUTED NOISE N(0,1)\n",
      "play(balance_signal(signal), sr) # normalize output so that it is in the range [-1,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Listen at a lower sample rate (8000Hz), which means a Nyquist frequency of 4kHz, the highest frequency of the audio\n",
      "sr = 8000 # change the sample rate to a lower rate\n",
      "dur = 2.0 # seconds\n",
      "signal = randn(sr * dur) # generate dur seconds of NORMALLY DISTRIBUTED NOISE N(0,1)\n",
      "play(balance_signal(signal), sr) # normalize output so that it is in the range [-1,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Listen to UNIFORM RANDOM NOISE, we must center it by subtracting the mean\n",
      "sr = 44100\n",
      "dur = 2.0 # seconds\n",
      "signal = rand(sr * dur) # dur seconds of UNIFORM RANDOM NOISE [0,1)\n",
      "play(balance_signal(signal-signal.mean()), sr) # normalize output so that it is in the range [-1,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Listen to POISSON (SHOT) NOISE, we must center it by subtracting the mean\n",
      "# Use scipy.random.poisson? or help(scipy.random.poisson) to get info on the function\n",
      "sr = 44100\n",
      "dur = 2.0 # seconds\n",
      "signal = scipy.random.poisson(0.001, sr * dur) # dur seconds of POISSON (SHOT) NOISE with parameter 0.001\n",
      "play(balance_signal(signal-signal.mean()), sr) # normalize output so that it is in the range [-1,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Listen to POISSON (SHOT) NOISE, we must center it by subtracting the mean\n",
      "sr = 44100\n",
      "dur = 2.0 # seconds\n",
      "signal = scipy.random.poisson(0.01, sr * dur) # dur seconds of POISSON (SHOT) NOISE with parameter 0.01\n",
      "play(balance_signal(signal-signal.mean()), sr) # normalize output so that it is in the range [-1,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Saving data as a WAV audio file\n",
      "   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save as a monoaural (1-channel MONO) audio file named \"poisson01.wav\" in the current notebook's directory\n",
      "wavwrite(balance_signal(signal-signal.mean()), 'poisson01.wav', sr) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Additive synthesis: freq, amp, phase, harmonics, partials\n",
      "\n",
      "The first and most fundamental computer audio synthesis method is called additive synthesis. We begin with the simplest periodic function, the sinusoid."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "play(sinusoid())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's take a look at the signal, with f0=441 there are 441 cycles per second with 100 samples per cycle\n",
      "signal = sinusoid(f0=441, num_points=251) # fundamental frequency 441 Hz, make 251 points of signal\n",
      "plot(signal); grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Phase can also be controlled, although that is not important for tones with just a few sinusoids\n",
      "plot(sinusoid(f0=441, num_points=151, phase_offset=0.5*pi)); grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can make a sound with multiple sinusoids. Sounds with harmonically related frequencies (1*f0, 2*f0, 3*f0, ...) perceptually\n",
      "# fuse into a single timbre\n",
      "signal = harmonics(num_harmonics=5)\n",
      "play(signal)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# When we plot the signal (waveform) we can see the multiple harmonics\n",
      "# The five harmonics are given random phase offsets by default\n",
      "# Try running this cell multiple times, what do you notice?\n",
      "signal = harmonics(f0=441, num_harmonics=5)\n",
      "plot(signal[:251]); grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Spectrum (Fourier Transform)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can plot the spectrum of a signal to see what its frequency content is\n",
      "# Here, we plot the Fourier transform to see how much of each frequency is present in the signal\n",
      "# We see that there are five clear horizontal lines, showing energy at five frequencies that are evenly spaced.\n",
      "LinearFrequencySpectrum(signal).feature_plot(normalize=True,dbscale=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The BregmanToolkit uses the following sets of parameters for its signal processing\n",
      "# Here we list them, so that you can try them in the method calls above:\n",
      "\n",
      "print \"List signal_params: used for sinusoid(), harmonics(), shepard()\"\n",
      "print default_signal_params()\n",
      "print \n",
      "print \"List noise_params: used for noise()\"\n",
      "print default_noise_params()\n",
      "print\n",
      "print \"List audio feature analysis params: used for LinearFrequencySpectrum(), LogFrequencySpectrum()\"\n",
      "print default_feature_params()\n",
      "print\n",
      "print \"You use the help() function to get more information on any object or method.\"\n",
      "help (testsignal.sinusoid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Using Sound to Explore Data\n",
      "## Loading the Example Sounds\n",
      "This homework includes sound examples for you to try. Loading a sound file requires specifying its path and filename. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the path to an audio file\n",
      "# In this case, the sound was generated from helioseismology data from our sun \n",
      "# The sound depicts the resonant modes of the sun's ringing.\n",
      "# The data has been vastly sped up from hours or days per cycle to fractions of seconds, by setting the sample rate to 44100Hz \n",
      "audio_file = os.path.join(\"sounds\",\"three_modes.wav\")\n",
      "print audio_file\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Playing and viewing audio files\n",
      "\n",
      "The BregmanToolkit has a built-in `play()` function for listening to audio data.\n",
      "Use the `balance_signal()` function to normalize the audio to allowable `(-1.0, 1.0)` range.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load and play the audio data\n",
      "snd,sr,fmt = wavread(audio_file) # load the audio file, giving the data (snd), sample rate (sr) and format\n",
      "print \"sr=\",sr\n",
      "print \"snd.shape:\", snd.shape, \"time:\",len(snd)/sr,\"secs\"  # print the audio data's array size\n",
      "print \"root mean square:\", sqrt(abs(snd**2).mean()) # root mean square \n",
      "print \"snd.min(), snd.max()\", snd.min(), \",\", snd.max()\n",
      "sys.stdout.flush() # print info before continuing\n",
      "\n",
      "play(snd,sr) # play it\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Play sound at a much slower rate, sr/4, which scales frequency content down by a factor of 4 \n",
      "play(snd[:len(snd)/4],sr/4) # truncate sound to 1/4 duration, otherwise will last 4 x longer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Play sound at a much faster rate, 4x, which scales frequency content up by a factor of 4 \n",
      "play(snd,sr*4) # sound is a quarter original duration (played 4 x faster)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the first 0.25s of the audio\n",
      "plot(arange(0.25*sr)/sr, snd[:0.25*sr])\n",
      "title('Sun: Helioseismic Time Series Data')\n",
      "xlabel('Time (s)')\n",
      "ylabel('Amplitude')\n",
      "grid()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the first 0.01s of the audio\n",
      "plot(arange(0.01*sr)/sr, snd[:0.01*sr])\n",
      "title('Sun: Helioseismic Time Series Data')\n",
      "xlabel('Time (s)')\n",
      "ylabel('Amplitude')\n",
      "grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Extracting and Plotting Features: The Spectrum\n",
      "\n",
      "The spectrum of a time series (such as sound data) is a very useful way of inspecting data. The spectrum consists of samples of the frequency content (how much energy there is in the data for a range of frequencies) over time, hence it is two dimensional. \n",
      "\n",
      "We are generally interested in the magnitude spectrum (often denoted by `spec.X`), there is also the phase spectrum. We can extract the full complex spectrum (magnitude and phase) using the short-time Fourier transform (`spec.STFT`).\n",
      "\n",
      "The following introductory example shows how: \n",
      "    - assign to a variable `linspec` the spectrogram of an audio file ( the Short-Time Fourier Transform (STFT) )\n",
      "    - specify STFT window parameters N,W, and H \n",
      "    - plot the result as a time-frequency image.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Short-time Fourier transform\n",
      "linspec = LinearFrequencySpectrum(audio_file, nfft=1024, wfft=512, nhop=256) # nfft=fft window, wfft=audio window, hop=skip size\n",
      "linspec.feature_plot(normalize=True,dbscale=True) # plot by first scaling the data to full range decibels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Looking at the time-average of the spectrum is often informative,\n",
      "# this is the view of the average spectrum taken over all time samples \n",
      "plot(linspec._fftfrqs, 20*log10(linspec.X.mean(1))) # x-axis is frequency, y-axis is power in dB\n",
      "axis('tight');grid() # Make the plot fit the image, tightly\n",
      "title('Average Spectrum of Time Series (log Power)')\n",
      "xlabel('Frequency (Hz)')\n",
      "ylabel('Power (dB)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Built-in Help on modules, objects, and their methods\n",
      "\n",
      "All BregmanToolkit objects are fully self documenting. \n",
      "\n",
      "`help(object)`\n",
      "\n",
      "Here, we'll access help on the spectrum object's feature_plot method,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(linspec.feature_plot)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Audio Feature Synthesis, Using the Spectrum to Sonify Data\n",
      "Bregman's Features class can synthesize audio from feature parameters. First, we use the Short-Time Fourier Transform to create a spectrogram. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Short-Time Fourier Transform (STFT)\n",
      "# Use phase-vocoder optimal 4:2:1 pattern for N, W, and H FFT parameters\n",
      "linspec = LinearFrequencySpectrum(audio_file, nfft=4096, wfft=2048, nhop=1024)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Time-Stretch / Expand (using the Phase Vocoder method)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Varispeed playback at 1/4 speed, also truncate generated audio to 1/3 length for brevity  \n",
      "# We achieve this by inverting the short-time Fourier transform using Feature.inverse() method\n",
      "# NOTE: rate of spectrum slices in time has changed, only the phases are modified\n",
      "x_hat = linspec.inverse(pvoc=0.25) # invert features 1/4 speed using a phase vocoder with windowing\n",
      "x_len = len(x_hat) // 3  # double slash means perform integer division (truncate decimals).\n",
      "play(balance_signal(x_hat[:x_len]),sr) # play 1/3 of the audio generated by inverse(pvoc=0.25) above"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Varispeed playback at 4 x speed  \n",
      "# We achieve this by inverting the short-time Fourier transform using Feature.inverse() method\n",
      "# NOTE: rate of spectrum slices in time has changed, only the phases are modified\n",
      "x_hat = linspec.inverse(pvoc=4.0) # invert features 4 x speed using a phase vocoder with windowing\n",
      "x_len = len(x_hat)  # double slash means perform integer division (truncate decimals).\n",
      "play(balance_signal(x_hat),sr) # play audio generated by inverse(pvoc=0.25) above"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using the Inverse Fourier Transform to Sonify Image Data\n",
      "\n",
      "Here we will load an image and use the image data as STFT coefficients via LinearFrequencySpectrum, invert the spectrum to sonify the image.\n",
      "\n",
      "Setting Fourier transform coefficients with data, and using the inverse() method, yields sonifications of images. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import Image # Use Python's image library"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "im = Image.open(\"SDSS_1600.jpg\") # Load the provided SDSS data, on the large-scale structure of the universe\n",
      "imshow(im) # we can display 2D data as an image, just like we did with the Spectrum above\n",
      "print im.format, im.size, im.mode # inspect the dimensions and format of the image\n",
      "title('Spatial Galaxy Density v Redshift (Distance), source: SDSS')\n",
      "xlabel('Image X')\n",
      "ylabel('Image Y')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert Image to Grayscale in the range [0.0,1.0]\n",
      "data = array(im.getdata()) / 255.\n",
      "data = data.reshape(im.size[1],im.size[0],3) # imWidth x imHeight x 3 RGB channels\n",
      "imshow(data.mean(2), cmap=cm.gray)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The image_to_spectrum method takes 2D array data and maps it to an audio spectrum\n",
      "help(image_to_spectrum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we can convert an image to a spectrum, and invert the spectrum to audio\n",
      "# This method is a form of additive synthesis, but using localized windowed 'grains' of sound scattered in Freq and Time. \n",
      "# It is called granular synthesis.\n",
      "spec = image_to_spectrum(data)\n",
      "spec.feature_plot(normalize=True, cmap=cm.gray)\n",
      "snd = spec.inverse()\n",
      "print \"snd duration=\", len(snd)/float(sr), \"(s)\"\n",
      "show()\n",
      "play(balance_signal(snd),sr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can access individual color channels of an image\n",
      "help(extract_color)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subplot(221); imshow(extract_color(data, 0)); title('R')\n",
      "subplot(222); imshow(extract_color(data, 1)); title('G')\n",
      "subplot(223); imshow(extract_color(data, 2)); title('B')\n",
      "subplot(224); imshow(data); title('RGB')\n",
      "show()\n",
      "\n",
      "print \"RED\"\n",
      "universe_spec_r = image_to_spectrum(extract_color(data, 0))\n",
      "r_hat = universe_spec_r.inverse()\n",
      "play(balance_signal(r_hat),sr)\n",
      "\n",
      "print \"GREEN\"\n",
      "universe_spec_g = image_to_spectrum(extract_color(data, 1))\n",
      "g_hat = universe_spec_g.inverse()\n",
      "play(balance_signal(g_hat),sr)\n",
      "\n",
      "print \"BLUE\"\n",
      "universe_spec_b = image_to_spectrum(extract_color(data, 2))\n",
      "b_hat = universe_spec_b.inverse()\n",
      "play(balance_signal(b_hat),sr)\n",
      "\n",
      "print \"RGB\"\n",
      "play(balance_signal(r_hat+g_hat+b_hat),sr)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Log Frequency Spectrum\n",
      "\n",
      "We can repeat the same operation as above, using a logarithmic frequency axis, which closer represents human perception of spectrum frequencies than linear frequency scaling.\n",
      "\n",
      "The frequencies bands are now spaced increasingly further apart, with most of the information concentrated in the low frequency spectrum, and progressively less information about higher frequencies (as with our ears).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract a Log-frequency spectrum, specifying windowing parameters\"\n",
      "logspec = LogFrequencySpectrum(audio_file, nhop=2205) # extract log spectrum\n",
      "logspec.feature_plot(dbscale=True, cmap=cm.gray) # plot features on dB scale\n",
      "title('Narrow-band Log Spectrum')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Just as with the Linear spectrum, we can invert the log spectrum using the feature inverse() method\"\n",
      "x_hat = logspec.inverse() # invert phaseless features to audio\n",
      "play(balance_signal(x_hat)) # play inverted features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Using the Inverse Log Frequency Spectrum to Sonify Image Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# By using a longer FFT window, we generate more samples in time\n",
      "spec = image_to_logspectrum(data, nfft=4096)\n",
      "spec.feature_plot(normalize=True, cmap=cm.gray)\n",
      "snd = spec.inverse()\n",
      "show()\n",
      "play(balance_signal(snd))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# List the (default) parameters that control feature extraction.\n",
      "# Use any parameter as a keyword argument to a feature extractor.\n",
      "# Or use a parameter dict {'key1':value1, ...} with the base Feature class\n",
      "\n",
      "p = Features.default_params() # inspect default parameters\n",
      "for parameter in p: print parameter+': ', p[parameter] # show feature extraction parameter dict\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Mapping Data to Musical Parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Musical Maps    \n",
      "    - Pitch\n",
      "        - Scale \n",
      "        - Degree \n",
      "        - Mode (Rotation)\n",
      "    - Rhythm and Meter\n",
      "        - Duration\n",
      "        - Accent\n",
      "        - Metrical Hierarchy\n",
      "    - Harmony\n",
      "        - The circle of fifths\n",
      "        - Modulation(Key)\n",
      "        - Combining pitches to make chords"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Making a computer musical instrument (CMI)\n",
      "\n",
      "To make a computer music instrument, a simple method is to use the harmonics() function, specify the frequency\n",
      "as musical pitch class offset from A440 (note), amplitude, and duration (in seconds)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(ds) # get help on our datascapes utility functions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Specify a note using pitch (in half steps [base_freq=440 by default]), amplitude [0.0 - 1.0], and duration (seconds)  \n",
      "play(make_note(0, 0.5, 1.0) ) \n",
      "plot(make_note(0,0.5,.01)); grid() # plot 1/100th second of note"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make a list of sounds\n",
      "note_list = [] # an empty list\n",
      "note_list.append(make_note(0, 0.5, 1.0))\n",
      "note_list.append(make_note(2, 0.5, 1.0))\n",
      "note_list.append(make_note(4, 0.5, 1.0))\n",
      "note_list.append(make_note(0, 0.5, 1.0))\n",
      "melody = hstack(note_list) # horizontal stacking of the list of note signals into a melody\n",
      "play(melody)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Short cut using Python's \"list comprehension\" notation\n",
      "melody = hstack([make_note(n, 0.5, 0.5) for n in [0,2,4,0,0,2,4,0]])\n",
      "play(melody)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Multiple notes can be added to make a multi-part melody\n",
      "melody = hstack([make_note(n, 0.5, 0.5) for n in [0,2,4,0,0,2,4,0]])\n",
      "bass = hstack([make_note(n, 0.5, 0.5, base_freq=220.) for n in [0,-5,0,0,0,-5,0,0]])\n",
      "play(melody + bass)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can change the timbre of each note by changing the number of harmonics\n",
      "melody = hstack([make_note(0, 0.5, 0.5, base_freq=220, num_harmonics=h) for h in [1, 2, 3, 7, 11, 5, 3, 1]])\n",
      "play(melody)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Mapping to a Musical Scales and Modes\n",
      "\n",
      "In the above example, we treat the data as though pitches were organized in 1/2 steps (as adjacent notes on the piano). However, musicians think in musical scales. Musical scales select a subset of the 12 chromatic pitch classes, indexing them by half-step offets. To specify a scale, make a list of half-step offsets within the octave."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "melody = hstack([make_note(mode_map(n, mode=modes['major']), 0.5, 0.5) for n in [0,1,2,0,0,1,2,0]])\n",
      "play(melody)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "melody = hstack([make_note(mode_map(n, mode=modes['minor']), 0.5, 0.5) for n in [0,1,2,0,0,1,2,0]])\n",
      "play(melody)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Python comments begin with '#' and are ignored by the interpreter\n",
      "# Here we load the kp-index data into a pandas DataFrame structure\n",
      "kp_data = pd.read_csv('spidr_1451721279326_0.txt', sep=' ', skiprows=15, skipinitialspace=True, parse_dates={'datetime': [0, 1]}, usecols=[0,1,2], index_col=[0]) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Multiply kpdata by 3 to get integers (0..29) and resample time-series to 1 year means\n",
      "kp_year = map_pitch( kp_data*3.0, rule='12m', how='mean', mode=modes['major'])\n",
      "stem(kp_year['value'].get_values())\n",
      "title('KP-Index resampled to year data', fontsize=18)\n",
      "xlabel('Year', fontsize=16)\n",
      "ylabel('Mean KP-Index', fontsize=16)\n",
      "grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(make_note)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Multiply map the data into sound using mapped pitch class (note), amp, dur, and num_harmonics simultaneously \n",
      "notes = [make_note(note, amp=note/20., dur=note/100.0, base_freq=110., num_harmoics=note) for note in kp_year['value']]\n",
      "snd = hstack(notes) # concatenate all of the notes into an audio signal\n",
      "print \"snd duration=\", len(snd)/44100., \"(s)\"\n",
      "play(snd, 44100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## ASSIGNMENT : Mapping from TimeSeries Data to Music via Additive Synthesis\n",
      "\n",
      "Finally, we can map from time series data (such as kp-index data) to a musical sequence. You worked in Short Assignment 1 on methods to reduce the time series into mean and median values over various time scales. We can take those values, map them to integers by multiplying by 3 (remember Earth KP-Index data is quantized into 1/3rds), and map via a musical scale to generate notes.\n",
      "\n",
      "Durations can be set to vary based on value also, as well as amplitude, and timbre (via the num_harmonics parameter). Your assignment for this week is to choose a dataset, either multiple images from the Sloan Digital Sky Survey, or multiple time series data from Solar WIND data, or Earth's Magnetic Field data (KP-Index) and to use various combinations of the above described methods to make creative sonic and visual materials by mapping.\n",
      "\n",
      "- Criteria:\n",
      "    - At least 3 different sonificiations and/or visualizations of data provided in the GitHug\n",
      "        - Universe and Earth data are acceptible\n",
      "    - The data must be processed via three different mappings, and you must describe your mapping choices\n",
      "        - Mapping can be chosen from the processes discussed above, or of your choosing, but you must describe it. \n",
      "    - The sonifiations / visualizations must be saved as series of image, or as a sound file, or a movie.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SUMMARY OF WEEK 1 Python Topics: Data Acquisition, Parsing, Filtering, Mining\n",
      "\n",
      "- Data Acquisition and Parsing  \n",
      "    - Making a time series\n",
      "    - Making a DataFrame\n",
      "    - Loading and parsing a DataFame\n",
      "    - Extracting the time series\n",
      "    - Loading different formats\n",
      "        - csv, tsv, text\n",
      "        - `.hd5,.cdf,.fits`:  file formats;`.tar:archive;.gz,.bz2,.7z,.z`:  compression\n",
      "\n",
      "- Accessing, Filtering, and Plotting Data\n",
      "     - slicing and dicing\n",
      "        - range `[start:stop]`, skip `[start:stop:skip]` \n",
      "     - integer indexing `series[0:10]`\n",
      "        - basic plot `plot(series)`\n",
      "        - histogram `histogram(series, bins)`\n",
      "    - string indexing \n",
      "        - scatter plot `plot(series1, series2, 'o')`\n",
      "    - object indexing\n",
      "        - imshow 2D map `imshow(series2D, colormap=cm.jet)`\n",
      "\n",
      "    - multi-column indexing\n",
      "        - `series.loc[column1, column2]`\n",
      "\n",
      "- Saving your Data\n",
      "    - csv\n",
      "    - json\n",
      "    - pickle\n",
      "    - audio \n",
      "    - image\n",
      "    - movie\n",
      "    \n",
      "- Data Wrangling  \n",
      "    - Centering and scaling \n",
      "    - Normalization and Whitening\n",
      "    - Quantization\n",
      "    - Derivatives\n",
      "    - Resampling\n",
      "    - Spectrum: Fourier transform\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Ben Fry's Seven Stages of Visualizing Data</h2>\n",
      "<img src=\"BenFry_SevenStagesVisualization.png\">\n",
      "Source:  \n",
      "Freg, B. *Visualizing Data*, O'Reilly, 2008. NOTE: It's worth googling around for this book.\n",
      "\n",
      "\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}